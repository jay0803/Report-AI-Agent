"""
Bulk Daily Report Ingestion Script

backend/Data/mock_reports/daily í´ë”ì˜ ëª¨ë“  txt íŒŒì¼ì„ ì½ì–´ì„œ
PostgreSQLì˜ daily_reports í…Œì´ë¸”ì— ì €ì¥í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸

Usage:
    python backend/tools/bulk_daily_ingest.py
"""
import sys
import os
import json
import re
from pathlib import Path
from datetime import datetime, date
from typing import List, Dict, Any, Optional

# Windows ì½˜ì†” UTF-8 ì„¤ì •
if sys.platform == 'win32':
    os.system('chcp 65001 >nul 2>&1')
    sys.stdout.reconfigure(encoding='utf-8') if hasattr(sys.stdout, 'reconfigure') else None

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python pathì— ì¶”ê°€
backend_dir = Path(__file__).parent.parent
sys.path.insert(0, str(backend_dir))

from app.infrastructure.database.session import SessionLocal
from app.domain.report.daily.repository import DailyReportRepository
from app.domain.report.daily.schemas import DailyReportCreate
from app.domain.report.core.canonical_models import CanonicalReport, CanonicalDaily, DetailTask
import uuid


def parse_time_range(time_str: str) -> tuple[Optional[str], Optional[str]]:
    """
    ì‹œê°„ ë²”ìœ„ ë¬¸ìì—´ì„ íŒŒì‹±í•˜ì—¬ (start, end) íŠœí”Œ ë°˜í™˜
    
    ì˜ˆ: "09:00 - 10:00" -> ("09:00", "10:00")
    
    Args:
        time_str: ì‹œê°„ ë²”ìœ„ ë¬¸ìì—´
        
    Returns:
        (time_start, time_end) íŠœí”Œ
    """
    if not time_str or time_str.strip() == "":
        return (None, None)
    
    # "09:00 - 10:00" íŒ¨í„´ ë§¤ì¹­
    match = re.match(r'(\d{1,2}:\d{2})\s*-\s*(\d{1,2}:\d{2})', time_str.strip())
    if match:
        return (match.group(1), match.group(2))
    
    # ë‹¨ì¼ ì‹œê°„ë§Œ ìˆëŠ” ê²½ìš° (ì˜ˆ: "09:00")
    match = re.match(r'(\d{1,2}:\d{2})', time_str.strip())
    if match:
        return (match.group(1), None)
    
    return (None, None)


def parse_date(date_str: str) -> date:
    """
    ë‚ ì§œ ë¬¸ìì—´ì„ date ê°ì²´ë¡œ ë³€í™˜
    
    ì˜ˆ: "2025-01-02" -> date(2025, 1, 2)
    
    Args:
        date_str: ë‚ ì§œ ë¬¸ìì—´ (YYYY-MM-DD)
        
    Returns:
        date ê°ì²´
    """
    try:
        return datetime.strptime(date_str, "%Y-%m-%d").date()
    except ValueError as e:
        raise ValueError(f"ë‚ ì§œ í˜•ì‹ ì˜¤ë¥˜: {date_str}. YYYY-MM-DD í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤. ({e})")


def convert_to_canonical_report(raw_json: Dict[str, Any]) -> CanonicalReport:
    """
    Raw JSONì„ CanonicalReportë¡œ ë³€í™˜
    
    Args:
        raw_json: ì›ë³¸ JSON ë”•ì…”ë„ˆë¦¬
        
    Returns:
        CanonicalReport ê°ì²´
    """
    # 1. ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
    ì‘ì„±ì¼ì = raw_json["ìƒë‹¨ì •ë³´"]["ì‘ì„±ì¼ì"]
    ì„±ëª… = raw_json["ìƒë‹¨ì •ë³´"]["ì„±ëª…"]
    
    period_date = parse_date(ì‘ì„±ì¼ì)
    
    # 2. í—¤ë” ì •ë³´
    header = {
        "ì‘ì„±ì¼ì": ì‘ì„±ì¼ì,
        "ì„±ëª…": ì„±ëª…
    }
    
    # 3. summary_tasks (ê¸ˆì¼_ì§„í–‰_ì—…ë¬´)
    summary_tasks = []
    ê¸ˆì¼ì§„í–‰ì—…ë¬´ = raw_json.get("ê¸ˆì¼_ì§„í–‰_ì—…ë¬´", "")
    if ê¸ˆì¼ì§„í–‰ì—…ë¬´:
        if isinstance(ê¸ˆì¼ì§„í–‰ì—…ë¬´, list):
            summary_tasks = ê¸ˆì¼ì§„í–‰ì—…ë¬´
        else:
            summary_tasks = [ê¸ˆì¼ì§„í–‰ì—…ë¬´] if ê¸ˆì¼ì§„í–‰ì—…ë¬´.strip() else []
    
    # 4. detail_tasks (ì„¸ë¶€ì—…ë¬´)
    detail_tasks = []
    ì„¸ë¶€ì—…ë¬´ = raw_json.get("ì„¸ë¶€ì—…ë¬´", [])
    for task_data in ì„¸ë¶€ì—…ë¬´:
        ì—…ë¬´ë‚´ìš© = task_data.get("ì—…ë¬´ë‚´ìš©", "")
        if not ì—…ë¬´ë‚´ìš© or not ì—…ë¬´ë‚´ìš©.strip():
            continue
        
        time_str = task_data.get("ì‹œê°„", "")
        time_start, time_end = parse_time_range(time_str)
        
        detail_task = DetailTask(
            time_start=time_start,
            time_end=time_end,
            text=ì—…ë¬´ë‚´ìš©,
            note=task_data.get("ë¹„ê³ ", "")
        )
        detail_tasks.append(detail_task)
    
    # 5. pending (ë¯¸ì¢…ê²°_ì—…ë¬´ì‚¬í•­)
    pending = []
    ë¯¸ì¢…ê²° = raw_json.get("ë¯¸ì¢…ê²°_ì—…ë¬´ì‚¬í•­", "")
    if ë¯¸ì¢…ê²°:
        if isinstance(ë¯¸ì¢…ê²°, list):
            pending = ë¯¸ì¢…ê²°
        else:
            pending = [ë¯¸ì¢…ê²°] if ë¯¸ì¢…ê²°.strip() else []
    
    # 6. plans (ìµì¼_ì—…ë¬´ê³„íš)
    plans = []
    ìµì¼ê³„íš = raw_json.get("ìµì¼_ì—…ë¬´ê³„íš", "")
    if ìµì¼ê³„íš:
        if isinstance(ìµì¼ê³„íš, list):
            plans = ìµì¼ê³„íš
        else:
            plans = [ìµì¼ê³„íš] if ìµì¼ê³„íš.strip() else []
    
    # 7. notes (íŠ¹ì´ì‚¬í•­)
    notes = raw_json.get("íŠ¹ì´ì‚¬í•­", "") or ""
    
    # 8. CanonicalDaily ìƒì„±
    canonical_daily = CanonicalDaily(
        header=header,
        summary_tasks=summary_tasks,
        detail_tasks=detail_tasks,
        pending=pending,
        plans=plans,
        notes=notes
    )
    
    # 9. CanonicalReport ìƒì„±
    report = CanonicalReport(
        report_id=str(uuid.uuid4()),
        report_type="daily",
        owner=ì„±ëª…,
        period_start=period_date,
        period_end=period_date,
        daily=canonical_daily
    )
    
    return report


def read_json_objects_from_file(file_path: Path) -> List[Dict[str, Any]]:
    """
    txt íŒŒì¼ì—ì„œ ì—¬ëŸ¬ JSON ê°ì²´ë¥¼ ì½ì–´ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
    
    ê° JSON ê°ì²´ëŠ” ë¹ˆ ì¤„ë¡œ êµ¬ë¶„ë¨
    
    Args:
        file_path: txt íŒŒì¼ ê²½ë¡œ
        
    Returns:
        JSON ê°ì²´ ë¦¬ìŠ¤íŠ¸
    """
    json_objects = []
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # ë¹ˆ ì¤„ë¡œ ë¶„ë¦¬ëœ JSON ê°ì²´ë“¤ì„ ì¶”ì¶œ
        # ì¤‘ê´„í˜¸ë¡œ ì‹œì‘í•˜ê³  ëë‚˜ëŠ” íŒ¨í„´ ì°¾ê¸°
        json_texts = re.findall(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', content, re.DOTALL)
        
        for json_text in json_texts:
            try:
                obj = json.loads(json_text)
                json_objects.append(obj)
            except json.JSONDecodeError as e:
                print(f"âš ï¸  JSON íŒŒì‹± ì˜¤ë¥˜ ({file_path.name}): {e}")
                continue
    
    except Exception as e:
        print(f"âŒ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜ ({file_path}): {e}")
    
    return json_objects


def find_all_txt_files(base_dir: Path, year: Optional[int] = None, month: Optional[int] = None) -> List[Path]:
    """
    base_dir í•˜ìœ„ì˜ txt íŒŒì¼ ì°¾ê¸° (ë‚ ì§œ í•„í„°ë§ ì˜µì…˜)
    
    Args:
        base_dir: ê¸°ë³¸ ë””ë ‰í† ë¦¬
        year: í•„í„°ë§í•  ì—°ë„ (Noneì´ë©´ ëª¨ë“  ì—°ë„)
        month: í•„í„°ë§í•  ì›” (Noneì´ë©´ ëª¨ë“  ì›”, ì˜ˆ: 11 = 11ì›”)
        
    Returns:
        txt íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
    """
    all_files = list(base_dir.rglob("*.txt"))
    
    # ë‚ ì§œ í•„í„°ë§ì´ ì—†ìœ¼ë©´ ëª¨ë“  íŒŒì¼ ë°˜í™˜
    if year is None and month is None:
        return sorted(all_files)
    
    # ë‚ ì§œ í•„í„°ë§
    filtered_files = []
    for file_path in all_files:
        filename = file_path.stem  # í™•ì¥ì ì œê±°
        
        try:
            # YYYY-MM-DD í˜•ì‹ íŒŒì‹±
            parts = filename.split('-')
            if len(parts) >= 3:
                file_year = int(parts[0])
                file_month = int(parts[1])
                
                # í•„í„°ë§ ì¡°ê±´ í™•ì¸
                if year is not None and file_year != year:
                    continue
                if month is not None and file_month != month:
                    continue
                
                filtered_files.append(file_path)
        except (ValueError, IndexError):
            # ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨í•œ íŒŒì¼ì€ ì œì™¸
            continue
    
    return sorted(filtered_files)


def bulk_ingest_daily_reports(year: Optional[int] = None, month: Optional[int] = None):
    """
    ë©”ì¸ í•¨ìˆ˜: ì¼ì¼ë³´ê³ ì„œë¥¼ DBì— ì €ì¥
    
    Args:
        year: í•„í„°ë§í•  ì—°ë„ (Noneì´ë©´ ëª¨ë“  ì—°ë„)
        month: í•„í„°ë§í•  ì›” (Noneì´ë©´ ëª¨ë“  ì›”, ì˜ˆ: 11 = 11ì›”)
    """
    print("=" * 70)
    print("ğŸ“Š ì¼ì¼ë³´ê³ ì„œ Bulk Ingestion ì‹œì‘")
    if year or month:
        filter_msg = []
        if year:
            filter_msg.append(f"{year}ë…„")
        if month:
            filter_msg.append(f"{month}ì›”")
        print(f"í•„í„°: {' '.join(filter_msg)}")
    print("=" * 70)
    
    # 1. ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
    base_dir = backend_dir / "Data" / "mock_reports" / "daily"
    
    if not base_dir.exists():
        print(f"âŒ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {base_dir}")
        return
    
    print(f"\nğŸ“ ëŒ€ìƒ ë””ë ‰í† ë¦¬: {base_dir}")
    
    # 2. txt íŒŒì¼ ì°¾ê¸° (ë‚ ì§œ í•„í„°ë§ ì ìš©)
    txt_files = find_all_txt_files(base_dir, year=year, month=month)
    print(f"ğŸ“„ ë°œê²¬ëœ txt íŒŒì¼: {len(txt_files)}ê°œ")
    if year or month:
        print(f"   (í•„í„°: {year or 'ëª¨ë“  ì—°ë„'}ë…„ {month or 'ëª¨ë“  ì›”'}ì›”)")
    
    if not txt_files:
        print("âš ï¸  txt íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # 3. DB ì„¸ì…˜ ìƒì„±
    db = SessionLocal()
    
    # í†µê³„
    total_reports = 0
    created_count = 0
    updated_count = 0
    error_count = 0
    
    try:
        # 4. ê° íŒŒì¼ ì²˜ë¦¬
        for file_path in txt_files:
            print(f"\nğŸ“– ì²˜ë¦¬ ì¤‘: {file_path.relative_to(base_dir)}")
            
            # 4-1. íŒŒì¼ì—ì„œ JSON ê°ì²´ë“¤ ì½ê¸°
            json_objects = read_json_objects_from_file(file_path)
            print(f"   â”œâ”€ JSON ê°ì²´ ìˆ˜: {len(json_objects)}ê°œ")
            
            # 4-2. ê° JSON ê°ì²´ë¥¼ CanonicalReportë¡œ ë³€í™˜ í›„ DB ì €ì¥
            for idx, json_obj in enumerate(json_objects, 1):
                try:
                    # CanonicalReport ë³€í™˜
                    canonical_report = convert_to_canonical_report(json_obj)
                    
                    # DB ì €ì¥ (UPSERT)
                    report_dict = canonical_report.model_dump(mode='json')
                    report_create = DailyReportCreate(
                        owner=canonical_report.owner,
                        report_date=canonical_report.period_start,
                        report_json=report_dict
                    )
                    
                    db_report, is_created = DailyReportRepository.create_or_update(
                        db, report_create
                    )
                    
                    total_reports += 1
                    if is_created:
                        created_count += 1
                        action = "ìƒì„±"
                    else:
                        updated_count += 1
                        action = "ì—…ë°ì´íŠ¸"
                    
                    print(f"   â”œâ”€ [{idx}/{len(json_objects)}] {canonical_report.owner} - {canonical_report.period_start} ({action})")
                
                except Exception as e:
                    error_count += 1
                    print(f"   â”œâ”€ âŒ [{idx}/{len(json_objects)}] ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    continue
        
        # 5. ê²°ê³¼ ì¶œë ¥
        print(f"\n{'=' * 70}")
        print(f"âœ… Bulk Ingestion ì™„ë£Œ!")
        print(f"{'=' * 70}")
        print(f"ğŸ“Š ì²˜ë¦¬ ê²°ê³¼:")
        print(f"   â”œâ”€ ì´ ë³´ê³ ì„œ ìˆ˜: {total_reports}ê°œ")
        print(f"   â”œâ”€ ìƒì„±: {created_count}ê°œ")
        print(f"   â”œâ”€ ì—…ë°ì´íŠ¸: {updated_count}ê°œ")
        print(f"   â””â”€ ì—ëŸ¬: {error_count}ê°œ")
        
        # 6. DB í™•ì¸
        print(f"\nğŸ” DB í™•ì¸:")
        from app.domain.report.daily.models import DailyReport
        kim_reports = db.query(DailyReport).filter(
            DailyReport.owner == "ê¹€ë³´í—˜"
        ).count()
        print(f"   â””â”€ 'ê¹€ë³´í—˜'ì˜ ì¼ì¼ë³´ê³ ì„œ: {kim_reports}ê°œ")
        
    except Exception as e:
        print(f"\nâŒ ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        db.close()
        print(f"\n{'=' * 70}")


if __name__ == "__main__":
    # ëª¨ë“  ëª©ì—… ë°ì´í„°ë¥¼ PostgreSQLì— ì €ì¥
    bulk_ingest_daily_reports()

