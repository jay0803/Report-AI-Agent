"""
Bulk Daily Report Ingestion Script

backend/Data/mock_reports/daily í´ë”ì˜ ëª¨ë“  txt íŒŒì¼ì„ ì½ì–´ì„œ
PostgreSQLì˜ daily_reports í…Œì´ë¸”ì— ì €ì¥í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸

Usage:
    python backend/tools/bulk_daily_ingest.py
"""
import sys
import os
import json
import re
from pathlib import Path
from datetime import datetime, date
from typing import List, Dict, Any, Optional

# Windows ì½˜ì†” UTF-8 ì„¤ì •
if sys.platform == 'win32':
    os.system('chcp 65001 >nul 2>&1')
    sys.stdout.reconfigure(encoding='utf-8') if hasattr(sys.stdout, 'reconfigure') else None

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python pathì— ì¶”ê°€
backend_dir = Path(__file__).parent.parent
sys.path.insert(0, str(backend_dir))

from app.infrastructure.database.session import SessionLocal
from app.domain.daily.repository import DailyReportRepository
from app.domain.daily.schemas import DailyReportCreate
from app.domain.report.schemas import CanonicalReport, TaskItem
import uuid


def parse_time_range(time_str: str) -> tuple[Optional[str], Optional[str]]:
    """
    ì‹œê°„ ë²”ìœ„ ë¬¸ìì—´ì„ íŒŒì‹±í•˜ì—¬ (start, end) íŠœí”Œ ë°˜í™˜
    
    ì˜ˆ: "09:00 - 10:00" -> ("09:00", "10:00")
    
    Args:
        time_str: ì‹œê°„ ë²”ìœ„ ë¬¸ìì—´
        
    Returns:
        (time_start, time_end) íŠœí”Œ
    """
    if not time_str or time_str.strip() == "":
        return (None, None)
    
    # "09:00 - 10:00" íŒ¨í„´ ë§¤ì¹­
    match = re.match(r'(\d{1,2}:\d{2})\s*-\s*(\d{1,2}:\d{2})', time_str.strip())
    if match:
        return (match.group(1), match.group(2))
    
    # ë‹¨ì¼ ì‹œê°„ë§Œ ìˆëŠ” ê²½ìš° (ì˜ˆ: "09:00")
    match = re.match(r'(\d{1,2}:\d{2})', time_str.strip())
    if match:
        return (match.group(1), None)
    
    return (None, None)


def parse_date(date_str: str) -> date:
    """
    ë‚ ì§œ ë¬¸ìì—´ì„ date ê°ì²´ë¡œ ë³€í™˜
    
    ì˜ˆ: "2025-01-02" -> date(2025, 1, 2)
    
    Args:
        date_str: ë‚ ì§œ ë¬¸ìì—´ (YYYY-MM-DD)
        
    Returns:
        date ê°ì²´
    """
    try:
        return datetime.strptime(date_str, "%Y-%m-%d").date()
    except ValueError as e:
        raise ValueError(f"ë‚ ì§œ í˜•ì‹ ì˜¤ë¥˜: {date_str}. YYYY-MM-DD í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤. ({e})")


def convert_to_canonical_report(raw_json: Dict[str, Any]) -> CanonicalReport:
    """
    Raw JSONì„ CanonicalReportë¡œ ë³€í™˜
    
    Args:
        raw_json: ì›ë³¸ JSON ë”•ì…”ë„ˆë¦¬
        
    Returns:
        CanonicalReport ê°ì²´
    """
    # 1. ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
    ì‘ì„±ì¼ì = raw_json["ìƒë‹¨ì •ë³´"]["ì‘ì„±ì¼ì"]
    ì„±ëª… = raw_json["ìƒë‹¨ì •ë³´"]["ì„±ëª…"]
    
    period_date = parse_date(ì‘ì„±ì¼ì)
    
    # 2. ì„¸ë¶€ì—…ë¬´ë¥¼ TaskItemìœ¼ë¡œ ë³€í™˜
    tasks = []
    ì„¸ë¶€ì—…ë¬´ = raw_json.get("ì„¸ë¶€ì—…ë¬´", [])
    
    for idx, task_data in enumerate(ì„¸ë¶€ì—…ë¬´):
        time_str = task_data.get("ì‹œê°„", "")
        time_start, time_end = parse_time_range(time_str)
        
        task = TaskItem(
            task_id=f"time_{idx + 1}",
            title=task_data.get("ì—…ë¬´ë‚´ìš©", "").split()[0] if task_data.get("ì—…ë¬´ë‚´ìš©") else "ì—…ë¬´",
            description=task_data.get("ì—…ë¬´ë‚´ìš©", ""),
            time_start=time_start,
            time_end=time_end,
            status="ì™„ë£Œ",  # completed
            note=task_data.get("ë¹„ê³ ", "")
        )
        tasks.append(task)
    
    # 3. issues ì¶”ì¶œ
    issues = []
    ë¯¸ì¢…ê²° = raw_json.get("ë¯¸ì¢…ê²°_ì—…ë¬´ì‚¬í•­", "")
    if ë¯¸ì¢…ê²° and ë¯¸ì¢…ê²°.strip():
        issues.append(ë¯¸ì¢…ê²°)
    
    # 4. metadata ìƒì„±
    metadata = {}
    
    ìµì¼ê³„íš = raw_json.get("ìµì¼_ì—…ë¬´ê³„íš", "")
    if ìµì¼ê³„íš and ìµì¼ê³„íš.strip():
        metadata["next_plan"] = ìµì¼ê³„íš
    
    íŠ¹ì´ì‚¬í•­ = raw_json.get("íŠ¹ì´ì‚¬í•­", "")
    if íŠ¹ì´ì‚¬í•­ and íŠ¹ì´ì‚¬í•­.strip():
        metadata["notes"] = íŠ¹ì´ì‚¬í•­
    
    ê¸ˆì¼ì§„í–‰ì—…ë¬´ = raw_json.get("ê¸ˆì¼_ì§„í–‰_ì—…ë¬´", "")
    if ê¸ˆì¼ì§„í–‰ì—…ë¬´ and ê¸ˆì¼ì§„í–‰ì—…ë¬´.strip():
        metadata["summary"] = ê¸ˆì¼ì§„í–‰ì—…ë¬´
    
    # 5. CanonicalReport ìƒì„±
    report = CanonicalReport(
        report_id=str(uuid.uuid4()),
        report_type="daily",
        owner=ì„±ëª…,
        period_start=period_date,
        period_end=period_date,
        tasks=tasks,
        issues=issues,
        plans=[],  # ì¼ì¼ë³´ê³ ì„œì—ëŠ” plans ì—†ìŒ
        metadata=metadata
    )
    
    return report


def read_json_objects_from_file(file_path: Path) -> List[Dict[str, Any]]:
    """
    txt íŒŒì¼ì—ì„œ ì—¬ëŸ¬ JSON ê°ì²´ë¥¼ ì½ì–´ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
    
    ê° JSON ê°ì²´ëŠ” ë¹ˆ ì¤„ë¡œ êµ¬ë¶„ë¨
    
    Args:
        file_path: txt íŒŒì¼ ê²½ë¡œ
        
    Returns:
        JSON ê°ì²´ ë¦¬ìŠ¤íŠ¸
    """
    json_objects = []
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # ë¹ˆ ì¤„ë¡œ ë¶„ë¦¬ëœ JSON ê°ì²´ë“¤ì„ ì¶”ì¶œ
        # ì¤‘ê´„í˜¸ë¡œ ì‹œì‘í•˜ê³  ëë‚˜ëŠ” íŒ¨í„´ ì°¾ê¸°
        json_texts = re.findall(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', content, re.DOTALL)
        
        for json_text in json_texts:
            try:
                obj = json.loads(json_text)
                json_objects.append(obj)
            except json.JSONDecodeError as e:
                print(f"âš ï¸  JSON íŒŒì‹± ì˜¤ë¥˜ ({file_path.name}): {e}")
                continue
    
    except Exception as e:
        print(f"âŒ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜ ({file_path}): {e}")
    
    return json_objects


def find_all_txt_files(base_dir: Path) -> List[Path]:
    """
    base_dir í•˜ìœ„ì˜ ëª¨ë“  txt íŒŒì¼ ì°¾ê¸°
    
    Args:
        base_dir: ê¸°ë³¸ ë””ë ‰í† ë¦¬
        
    Returns:
        txt íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
    """
    return sorted(base_dir.rglob("*.txt"))


def bulk_ingest_daily_reports():
    """
    ë©”ì¸ í•¨ìˆ˜: ëª¨ë“  ì¼ì¼ë³´ê³ ì„œë¥¼ DBì— ì €ì¥
    """
    print("=" * 70)
    print("ğŸ“Š ì¼ì¼ë³´ê³ ì„œ Bulk Ingestion ì‹œì‘")
    print("=" * 70)
    
    # 1. ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
    base_dir = backend_dir / "Data" / "mock_reports" / "daily"
    
    if not base_dir.exists():
        print(f"âŒ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {base_dir}")
        return
    
    print(f"\nğŸ“ ëŒ€ìƒ ë””ë ‰í† ë¦¬: {base_dir}")
    
    # 2. ëª¨ë“  txt íŒŒì¼ ì°¾ê¸°
    txt_files = find_all_txt_files(base_dir)
    print(f"ğŸ“„ ë°œê²¬ëœ txt íŒŒì¼: {len(txt_files)}ê°œ")
    
    if not txt_files:
        print("âš ï¸  txt íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # 3. DB ì„¸ì…˜ ìƒì„±
    db = SessionLocal()
    
    # í†µê³„
    total_reports = 0
    created_count = 0
    updated_count = 0
    error_count = 0
    
    try:
        # 4. ê° íŒŒì¼ ì²˜ë¦¬
        for file_path in txt_files:
            print(f"\nğŸ“– ì²˜ë¦¬ ì¤‘: {file_path.relative_to(base_dir)}")
            
            # 4-1. íŒŒì¼ì—ì„œ JSON ê°ì²´ë“¤ ì½ê¸°
            json_objects = read_json_objects_from_file(file_path)
            print(f"   â”œâ”€ JSON ê°ì²´ ìˆ˜: {len(json_objects)}ê°œ")
            
            # 4-2. ê° JSON ê°ì²´ë¥¼ CanonicalReportë¡œ ë³€í™˜ í›„ DB ì €ì¥
            for idx, json_obj in enumerate(json_objects, 1):
                try:
                    # CanonicalReport ë³€í™˜
                    canonical_report = convert_to_canonical_report(json_obj)
                    
                    # DB ì €ì¥ (UPSERT)
                    report_dict = canonical_report.model_dump(mode='json')
                    report_create = DailyReportCreate(
                        owner=canonical_report.owner,
                        report_date=canonical_report.period_start,
                        report_json=report_dict
                    )
                    
                    db_report, is_created = DailyReportRepository.create_or_update(
                        db, report_create
                    )
                    
                    total_reports += 1
                    if is_created:
                        created_count += 1
                        action = "ìƒì„±"
                    else:
                        updated_count += 1
                        action = "ì—…ë°ì´íŠ¸"
                    
                    print(f"   â”œâ”€ [{idx}/{len(json_objects)}] {canonical_report.owner} - {canonical_report.period_start} ({action})")
                
                except Exception as e:
                    error_count += 1
                    print(f"   â”œâ”€ âŒ [{idx}/{len(json_objects)}] ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    continue
        
        # 5. ê²°ê³¼ ì¶œë ¥
        print(f"\n{'=' * 70}")
        print(f"âœ… Bulk Ingestion ì™„ë£Œ!")
        print(f"{'=' * 70}")
        print(f"ğŸ“Š ì²˜ë¦¬ ê²°ê³¼:")
        print(f"   â”œâ”€ ì´ ë³´ê³ ì„œ ìˆ˜: {total_reports}ê°œ")
        print(f"   â”œâ”€ ìƒì„±: {created_count}ê°œ")
        print(f"   â”œâ”€ ì—…ë°ì´íŠ¸: {updated_count}ê°œ")
        print(f"   â””â”€ ì—ëŸ¬: {error_count}ê°œ")
        
        # 6. DB í™•ì¸
        print(f"\nğŸ” DB í™•ì¸:")
        from app.domain.daily.models import DailyReport
        kim_reports = db.query(DailyReport).filter(
            DailyReport.owner == "ê¹€ë³´í—˜"
        ).count()
        print(f"   â””â”€ 'ê¹€ë³´í—˜'ì˜ ì¼ì¼ë³´ê³ ì„œ: {kim_reports}ê°œ")
        
    except Exception as e:
        print(f"\nâŒ ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        db.close()
        print(f"\n{'=' * 70}")


if __name__ == "__main__":
    bulk_ingest_daily_reports()

