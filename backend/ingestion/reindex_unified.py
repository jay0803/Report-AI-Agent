"""
í†µí•© Ingestion íŒŒì´í”„ë¼ì¸

ëª¨ë“  ë¬¸ì„œ íƒ€ì…ì„ UnifiedCanonicalë¡œ ë³€í™˜í•˜ì—¬ ë‹¨ì¼ ì»¬ë ‰ì…˜ì— ì €ì¥

ì²˜ë¦¬ ë¬¸ì„œ:
- ì¼ì¼ ë³´ê³ ì„œ (backend/Data/mock_reports/daily/*.txt)
- KPI ë¬¸ì„œ (output/*_kpi_canonical.json)
- ë³´ê³ ì„œ í…œí”Œë¦¿ (output/reports/*_canonical.json)

í”Œë¡œìš°:
1. íŒŒì¼ ìŠ¤ìº”
2. Raw â†’ CanonicalReport/CanonicalKPI ë³€í™˜ (ê¸°ì¡´ ë¡œì§)
3. Canonical â†’ UnifiedCanonical ë³€í™˜ (merge_normalizer)
4. UnifiedCanonical â†’ Chunks (unified_chunker)
5. Chunks â†’ Embeddings (OpenAI)
6. Embeddings â†’ Chroma (upsert)

Author: AI Assistant
Created: 2025-11-18

ì‚¬ìš©ë²•:
    python -m ingestion.reindex_unified
    python -m ingestion.reindex_unified --dry-run  # í…ŒìŠ¤íŠ¸ìš©
"""
import os
import sys
import json
import argparse
from pathlib import Path
from typing import List, Dict, Any

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì„¤ì •
project_root = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(project_root))

# .env ë¡œë“œ
try:
    from dotenv import load_dotenv
    env_path = project_root / ".env"
    if env_path.exists():
        load_dotenv(env_path)
        print(f"âœ… .env íŒŒì¼ ë¡œë“œë¨: {env_path}")
    # ë³´ê³ ì„œ ì „ìš© í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
    report_env_path = project_root / ".env.report"
    if report_env_path.exists():
        load_dotenv(report_env_path, override=False)
        print(f"âœ… .env.report íŒŒì¼ ë¡œë“œë¨: {report_env_path}")
except ImportError:
    print("âš ï¸  python-dotenvê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
except Exception as e:
    print(f"âš ï¸  .env íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {e}")

# ëª¨ë“ˆ ì„í¬íŠ¸
from app.domain.report.service import ReportProcessingService
from app.domain.report.schemas import CanonicalReport
from app.domain.kpi.schemas import CanonicalKPI
from app.domain.common.canonical_schema import UnifiedCanonical
from app.services.canonical.merge_normalizer import (
    report_to_unified,
    kpi_to_unified,
    text_to_unified
)
from app.domain.common.unified_chunker import chunk_unified, get_chunk_statistics
from ingestion.embed import embed_texts
from ingestion.chroma_client import get_chroma_service
from ingestion.ingest_daily_reports import (
    scan_mock_reports,
    parse_multi_json_file
)


# ========================================
# ì„¤ì •
# ========================================
DATA_DIR = project_root / "Data" / "mock_reports" / "daily"
OUTPUT_DIR = project_root / "output"
COLLECTION_NAME = "daily_reports_advanced"  # í†µí•© ì»¬ë ‰ì…˜
BATCH_SIZE = 100


# ========================================
# Step 1: ì¼ì¼ ë³´ê³ ì„œ ì²˜ë¦¬
# ========================================
def process_daily_reports(
    service: ReportProcessingService
) -> List[UnifiedCanonical]:
    """
    ì¼ì¼ ë³´ê³ ì„œ txt íŒŒì¼ â†’ UnifiedCanonical ë³€í™˜
    
    Args:
        service: ReportProcessingService ì¸ìŠ¤í„´ìŠ¤
        
    Returns:
        UnifiedCanonical ë¦¬ìŠ¤íŠ¸
    """
    print("=" * 80)
    print("ğŸ“Š Step 1: ì¼ì¼ ë³´ê³ ì„œ ì²˜ë¦¬")
    print("=" * 80)
    print()
    
    # íŒŒì¼ ìŠ¤ìº”
    print("â³ mock_reports í´ë” ìŠ¤ìº” ì¤‘...")
    file_infos = scan_mock_reports(DATA_DIR)
    
    if not file_infos:
        print("âŒ txt íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return []
    
    print(f"âœ… ì´ {len(file_infos)}ê°œ txt íŒŒì¼ ë°œê²¬")
    print()
    
    unified_docs = []
    total_reports = 0
    
    # ê° íŒŒì¼ ì²˜ë¦¬
    for idx, file_info in enumerate(file_infos):
        file_path = file_info["file_path"]
        relative_path = file_info["relative_path"]
        
        try:
            # JSON íŒŒì‹±
            json_objects = parse_multi_json_file(file_path)
            
            if not json_objects:
                continue
            
            total_reports += len(json_objects)
            
            # ê° JSON â†’ CanonicalReport â†’ UnifiedCanonical
            for json_obj in json_objects:
                try:
                    # Normalize (Raw â†’ CanonicalReport)
                    canonical_report = service.normalize_daily(json_obj)
                    
                    # ì†ŒìŠ¤ íŒŒì¼ ë©”íƒ€ë°ì´í„° ì¶”ê°€
                    canonical_report.metadata["source_file"] = relative_path
                    
                    # CanonicalReport â†’ UnifiedCanonical
                    unified = report_to_unified(canonical_report)
                    unified_docs.append(unified)
                    
                except Exception as e:
                    print(f"  âš ï¸  ë³´ê³ ì„œ ë³€í™˜ ì˜¤ë¥˜: {e}")
                    continue
            
            if (idx + 1) % 10 == 0:
                print(f"  ì§„í–‰: {idx + 1}/{len(file_infos)} íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ...")
        
        except Exception as e:
            print(f"  âŒ íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜ ({relative_path}): {e}")
            continue
    
    print()
    print(f"âœ… ì¼ì¼ ë³´ê³ ì„œ ì²˜ë¦¬ ì™„ë£Œ")
    print(f"   - ì´ íŒŒì¼: {len(file_infos)}ê°œ")
    print(f"   - ì´ ë³´ê³ ì„œ: {total_reports}ê°œ")
    print(f"   - UnifiedCanonical: {len(unified_docs)}ê°œ")
    print()
    
    return unified_docs


# ========================================
# Step 2: KPI ë¬¸ì„œ ì²˜ë¦¬
# ========================================
def process_kpi_documents() -> List[UnifiedCanonical]:
    """
    KPI canonical JSON íŒŒì¼ â†’ UnifiedCanonical ë³€í™˜
    
    Returns:
        UnifiedCanonical ë¦¬ìŠ¤íŠ¸
    """
    print("=" * 80)
    print("ğŸ“Š Step 2: KPI ë¬¸ì„œ ì²˜ë¦¬")
    print("=" * 80)
    print()
    
    unified_docs = []
    
    # KPI canonical íŒŒì¼ ì°¾ê¸°
    kpi_files = list(OUTPUT_DIR.glob("*_kpi_canonical.json"))
    
    if not kpi_files:
        print("âš ï¸  KPI canonical íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print()
        return []
    
    print(f"âœ… {len(kpi_files)}ê°œ KPI íŒŒì¼ ë°œê²¬")
    print()
    
    for kpi_file in kpi_files:
        try:
            with open(kpi_file, 'r', encoding='utf-8') as f:
                kpi_data = json.load(f)
            
            # CanonicalKPI ê°ì²´ ìƒì„±
            if isinstance(kpi_data, list):
                canonical_kpis = [CanonicalKPI(**item) for item in kpi_data]
            else:
                canonical_kpis = [CanonicalKPI(**kpi_data)]
            
            # CanonicalKPI â†’ UnifiedCanonical
            for canonical_kpi in canonical_kpis:
                try:
                    unified = kpi_to_unified(canonical_kpi)
                    unified.metadata["source_file"] = kpi_file.name
                    unified_docs.append(unified)
                except Exception as e:
                    print(f"  âš ï¸  KPI ë³€í™˜ ì˜¤ë¥˜: {e}")
                    continue
            
            print(f"  âœ… {kpi_file.name}: {len(canonical_kpis)}ê°œ KPI ë³€í™˜")
        
        except Exception as e:
            print(f"  âŒ íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜ ({kpi_file.name}): {e}")
            continue
    
    print()
    print(f"âœ… KPI ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ")
    print(f"   - UnifiedCanonical: {len(unified_docs)}ê°œ")
    print()
    
    return unified_docs


# ========================================
# Step 3: ë³´ê³ ì„œ í…œí”Œë¦¿ ì²˜ë¦¬
# ========================================
def process_report_templates() -> List[UnifiedCanonical]:
    """
    ë³´ê³ ì„œ í…œí”Œë¦¿ canonical JSON â†’ UnifiedCanonical ë³€í™˜
    
    Returns:
        UnifiedCanonical ë¦¬ìŠ¤íŠ¸
    """
    print("=" * 80)
    print("ğŸ“Š Step 3: ë³´ê³ ì„œ í…œí”Œë¦¿ ì²˜ë¦¬")
    print("=" * 80)
    print()
    
    unified_docs = []
    
    # í…œí”Œë¦¿ canonical íŒŒì¼ ì°¾ê¸° (kpi ì œì™¸)
    template_files = [
        f for f in OUTPUT_DIR.glob("*_canonical.json")
        if "kpi" not in f.name
    ]
    
    if not template_files:
        print("âš ï¸  í…œí”Œë¦¿ canonical íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print()
        return []
    
    print(f"âœ… {len(template_files)}ê°œ í…œí”Œë¦¿ íŒŒì¼ ë°œê²¬")
    print()
    
    for template_file in template_files:
        try:
            with open(template_file, 'r', encoding='utf-8') as f:
                template_data = json.load(f)
            
            # CanonicalReportë¡œ íŒŒì‹± ì‹œë„
            if isinstance(template_data, dict):
                try:
                    canonical_report = CanonicalReport(**template_data)
                    unified = report_to_unified(canonical_report)
                    unified.metadata["source_file"] = template_file.name
                    unified_docs.append(unified)
                    print(f"  âœ… {template_file.name}: ë³€í™˜ ì™„ë£Œ")
                except Exception as e:
                    print(f"  âš ï¸  {template_file.name}: CanonicalReport íŒŒì‹± ì‹¤íŒ¨, í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬")
                    # ì‹¤íŒ¨ ì‹œ raw textë¡œ ì²˜ë¦¬
                    raw_text = json.dumps(template_data, ensure_ascii=False, indent=2)
                    unified = text_to_unified(
                        text=raw_text,
                        title=template_file.stem,
                        source_file=template_file.name,
                        doc_type="template"
                    )
                    unified_docs.append(unified)
        
        except Exception as e:
            print(f"  âŒ íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜ ({template_file.name}): {e}")
            continue
    
    print()
    print(f"âœ… í…œí”Œë¦¿ ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ")
    print(f"   - UnifiedCanonical: {len(unified_docs)}ê°œ")
    print()
    
    return unified_docs


# ========================================
# Step 4: ì²­í‚¹ ë° ì„ë² ë”©
# ========================================
def process_chunks(
    unified_docs: List[UnifiedCanonical],
    api_key: str = None
) -> tuple[List[str], List[str], List[List[float]], List[Dict[str, Any]]]:
    """
    UnifiedCanonical â†’ Chunks â†’ Embeddings
    
    Args:
        unified_docs: UnifiedCanonical ë¦¬ìŠ¤íŠ¸
        api_key: OpenAI API í‚¤
        
    Returns:
        (ids, texts, embeddings, metadatas) íŠœí”Œ
    """
    print("=" * 80)
    print("ğŸ“Š Step 4: ì²­í‚¹ ë° ì„ë² ë”© ìƒì„±")
    print("=" * 80)
    print()
    
    all_chunks = []
    
    # ì²­í‚¹
    print("â³ ì²­í‚¹ ì¤‘...")
    for idx, unified in enumerate(unified_docs):
        try:
            chunks = chunk_unified(unified, include_summary=True)
            all_chunks.extend(chunks)
            
            if (idx + 1) % 50 == 0:
                print(f"  ì§„í–‰: {idx + 1}/{len(unified_docs)} ë¬¸ì„œ ì²­í‚¹ ì™„ë£Œ...")
        
        except Exception as e:
            print(f"  âš ï¸  ì²­í‚¹ ì˜¤ë¥˜ (doc_id: {unified.doc_id}): {e}")
            continue
    
    print(f"âœ… ì´ {len(all_chunks)}ê°œ ì²­í¬ ìƒì„±")
    print()
    
    # ì²­í¬ í†µê³„
    stats = get_chunk_statistics(all_chunks)
    print("ğŸ“Š ì²­í¬ í†µê³„:")
    print(f"  - ì´ ì²­í¬ ìˆ˜: {stats['total_chunks']}")
    print(f"  - ì²­í¬ íƒ€ì…ë³„:")
    for chunk_type, count in stats["chunk_types"].items():
        print(f"    â€¢ {chunk_type}: {count}")
    print(f"  - í‰ê·  í…ìŠ¤íŠ¸ ê¸¸ì´: {stats['avg_text_length']:.1f}ì")
    print()
    
    # ì„ë² ë”© ìƒì„±
    print("â³ ì„ë² ë”© ìƒì„± ì¤‘...")
    ids = [chunk["id"] for chunk in all_chunks]
    texts = [chunk["text"] for chunk in all_chunks]
    metadatas = [chunk["metadata"] for chunk in all_chunks]
    
    try:
        embeddings = embed_texts(texts, api_key=api_key, batch_size=BATCH_SIZE)
        print(f"âœ… {len(embeddings)}ê°œ ì„ë² ë”© ìƒì„± ì™„ë£Œ")
        print()
    except Exception as e:
        print(f"âŒ ì„ë² ë”© ìƒì„± ì˜¤ë¥˜: {e}")
        raise
    
    return ids, texts, embeddings, metadatas


# ========================================
# Step 5: Chroma ì—…ë¡œë“œ
# ========================================
def upload_to_chroma(
    ids: List[str],
    texts: List[str],
    embeddings: List[List[float]],
    metadatas: List[Dict[str, Any]],
    reset_collection: bool = True
):
    """
    ë¡œì»¬ ChromaDBì— ì—…ë¡œë“œ
    
    Args:
        ids: ì²­í¬ ID ë¦¬ìŠ¤íŠ¸
        texts: í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        embeddings: ì„ë² ë”© ë¦¬ìŠ¤íŠ¸
        metadatas: ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        reset_collection: Trueë©´ ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ í›„ ì¬ìƒì„±
    """
    print("=" * 80)
    print("ğŸ“Š Step 5: ë¡œì»¬ ChromaDB ì—…ë¡œë“œ")
    print("=" * 80)
    print()
    
    try:
        chroma_service = get_chroma_service()
        
        # ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ (ì˜µì…˜)
        if reset_collection:
            print(f"ğŸ—‘ï¸  ê¸°ì¡´ ì»¬ë ‰ì…˜ '{COLLECTION_NAME}' ì‚­ì œ ì¤‘...")
            try:
                chroma_service.delete_collection(name=COLLECTION_NAME)
                print(f"âœ… ì»¬ë ‰ì…˜ ì‚­ì œ ì™„ë£Œ")
            except Exception as e:
                print(f"âš ï¸  ì»¬ë ‰ì…˜ ì‚­ì œ ì‹¤íŒ¨ (ì¡´ì¬í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ): {e}")
            print()
        
        # ì»¬ë ‰ì…˜ ìƒì„±
        print(f"ğŸ“¦ ì»¬ë ‰ì…˜ '{COLLECTION_NAME}' ìƒì„± ì¤‘...")
        collection = chroma_service.get_or_create_collection(name=COLLECTION_NAME)
        print(f"âœ… ì»¬ë ‰ì…˜ ì¤€ë¹„ ì™„ë£Œ")
        print()
        
        # ë°°ì¹˜ ì—…ë¡œë“œ
        total = len(ids)
        print(f"â³ {total}ê°œ ë¬¸ì„œ ì—…ë¡œë“œ ì¤‘...")
        
        for i in range(0, total, BATCH_SIZE):
            batch_end = min(i + BATCH_SIZE, total)
            
            batch_ids = ids[i:batch_end]
            batch_embeddings = embeddings[i:batch_end]
            batch_documents = texts[i:batch_end]
            batch_metadatas = metadatas[i:batch_end]
            
            try:
                collection.upsert(
                    ids=batch_ids,
                    embeddings=batch_embeddings,
                    documents=batch_documents,
                    metadatas=batch_metadatas
                )
                print(f"  âœ… {i + 1}-{batch_end}/{total} ì—…ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                print(f"  âŒ ë°°ì¹˜ ì—…ë¡œë“œ ì˜¤ë¥˜ ({i}-{batch_end}): {e}")
                raise
        
        print()
        print("=" * 80)
        print("âœ… ë¡œì»¬ ChromaDB ì—…ë¡œë“œ ì™„ë£Œ!")
        print("=" * 80)
        print(f"ì»¬ë ‰ì…˜: {COLLECTION_NAME}")
        print(f"ì´ ë¬¸ì„œ ìˆ˜: {collection.count()}ê°œ")
        print()
    
    except Exception as e:
        print(f"âŒ ë¡œì»¬ ChromaDB ì˜¤ë¥˜: {e}")
        raise


# ========================================
# ë©”ì¸ íŒŒì´í”„ë¼ì¸
# ========================================
def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description="í†µí•© Ingestion íŒŒì´í”„ë¼ì¸"
    )
    parser.add_argument(
        "--api-key",
        type=str,
        default=None,
        help="OpenAI API í‚¤ (ê¸°ë³¸ê°’: í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEY)"
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Dry-run ëª¨ë“œ (Chroma ì—…ë¡œë“œ ì—†ì´ í†µê³„ë§Œ ì¶œë ¥)"
    )
    parser.add_argument(
        "--keep-collection",
        action="store_true",
        help="ê¸°ì¡´ ì»¬ë ‰ì…˜ ìœ ì§€ (ì‚­ì œí•˜ì§€ ì•ŠìŒ)"
    )
    
    args = parser.parse_args()
    
    # API í‚¤ í™•ì¸
    api_key = args.api_key or os.getenv("OPENAI_API_KEY")
    if not api_key and not args.dry_run:
        print("âŒ OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        print("   --api-key ì˜µì…˜ì„ ì‚¬ìš©í•˜ê±°ë‚˜ í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
        sys.exit(1)
    
    print()
    print("=" * 80)
    print("ğŸš€ í†µí•© Ingestion íŒŒì´í”„ë¼ì¸ ì‹œì‘")
    print("=" * 80)
    print()
    
    try:
        # ReportProcessingService ì´ˆê¸°í™”
        if args.dry_run:
            service = ReportProcessingService.__new__(ReportProcessingService)
            service.client = None
        else:
            service = ReportProcessingService(api_key=api_key)
        
        # Step 1: ì¼ì¼ ë³´ê³ ì„œ ì²˜ë¦¬
        daily_docs = process_daily_reports(service)
        
        # Step 2: KPI ë¬¸ì„œ ì²˜ë¦¬
        kpi_docs = process_kpi_documents()
        
        # Step 3: ë³´ê³ ì„œ í…œí”Œë¦¿ ì²˜ë¦¬
        template_docs = process_report_templates()
        
        # ì „ì²´ ë¬¸ì„œ í†µí•©
        all_unified_docs = daily_docs + kpi_docs + template_docs
        
        print("=" * 80)
        print("ğŸ“Š ì „ì²´ í†µê³„")
        print("=" * 80)
        print(f"ì¼ì¼ ë³´ê³ ì„œ: {len(daily_docs)}ê°œ")
        print(f"KPI ë¬¸ì„œ: {len(kpi_docs)}ê°œ")
        print(f"í…œí”Œë¦¿ ë¬¸ì„œ: {len(template_docs)}ê°œ")
        print(f"ì´ UnifiedCanonical: {len(all_unified_docs)}ê°œ")
        print()
        
        if not all_unified_docs:
            print("âŒ ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # Dry-run ì²´í¬
        if args.dry_run:
            print("ğŸ” Dry-run ëª¨ë“œ: Chroma ì—…ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return
        
        # Step 4: ì²­í‚¹ ë° ì„ë² ë”©
        ids, texts, embeddings, metadatas = process_chunks(
            all_unified_docs,
            api_key=api_key
        )
        
        # Step 5: Chroma ì—…ë¡œë“œ
        upload_to_chroma(
            ids, texts, embeddings, metadatas,
            reset_collection=not args.keep_collection
        )
        
        print("=" * 80)
        print("ğŸ‰ í†µí•© Ingestion ì™„ë£Œ!")
        print("=" * 80)
        print()
    
    except Exception as e:
        print()
        print("=" * 80)
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("=" * 80)
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()

