"""
ëª©ì—… ë°ì´í„° Ingestion ìŠ¤í¬ë¦½íŠ¸
ì¼ì¼/ì£¼ê°„/ì›”ê°„ ë³´ê³ ì„œ ëª©ì—… ë°ì´í„°ë¥¼ ì²­í‚¹, ì„ë² ë”©í•˜ì—¬ ChromaDBì— ì €ì¥

ì‚¬ìš©ë²•:
    python -m ingestion.ingest_mock_reports
"""
import os
import sys
import json
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import date
from dotenv import load_dotenv

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
project_root = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(project_root))

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv(project_root / ".env")
report_env_path = project_root / ".env.report"
if report_env_path.exists():
    load_dotenv(report_env_path, override=False)

from app.domain.report.core.service import ReportProcessingService
from app.domain.report.core.chunker import chunk_canonical_report
from app.domain.report.core.embedding_pipeline import EmbeddingPipeline
from app.infrastructure.vector_store_report import get_report_vector_store


# ë°ì´í„° ë””ë ‰í† ë¦¬
MOCK_DATA_DIR = project_root / "Data" / "mock_reports"
BATCH_SIZE = 100


def parse_json_file(file_path: Path) -> Optional[Dict[str, Any]]:
    """JSON íŒŒì¼ íŒŒì‹±"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read().strip()
        if not content:
            return None
        return json.loads(content)
    except json.JSONDecodeError as e:
        print(f"  âš ï¸  JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        return None
    except Exception as e:
        print(f"  âš ï¸  íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
        return None


def scan_report_files(report_type: str = "daily") -> List[Path]:
    """
    ë³´ê³ ì„œ íŒŒì¼ ìŠ¤ìº”
    
    Args:
        report_type: "daily", "weekly", "monthly"
    
    Returns:
        íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ (ë‚ ì§œ ìˆœ ì •ë ¬)
    """
    type_dir = MOCK_DATA_DIR / report_type
    
    if not type_dir.exists():
        print(f"âš ï¸  ë””ë ‰í† ë¦¬ ì—†ìŒ: {type_dir}")
        return []
    
    txt_files = list(type_dir.rglob("*.txt"))
    
    def extract_date(file_path: Path) -> tuple:
        """íŒŒì¼ëª…ì—ì„œ ë‚ ì§œ ì¶”ì¶œ (YYYY, MM, DD)"""
        filename = file_path.stem
        try:
            parts = filename.split('-')
            if len(parts) >= 3:
                year = int(parts[0])
                month = int(parts[1])
                day = int(parts[2])
                return (year, month, day)
        except (ValueError, IndexError):
            pass
        return (0, 0, 0)
    
    return sorted(txt_files, key=extract_date)


def ingest_daily_reports(
    service: ReportProcessingService,
    embedding_pipeline: EmbeddingPipeline,
    vector_store
) -> int:
    """ì¼ì¼ ë³´ê³ ì„œ ingestion"""
    print("\n" + "=" * 80)
    print("ğŸ“… ì¼ì¼ ë³´ê³ ì„œ Ingestion ì‹œì‘")
    print("=" * 80)
    
    txt_files = scan_report_files("daily")
    print(f"âœ… {len(txt_files)}ê°œ íŒŒì¼ ë°œê²¬\n")
    
    if not txt_files:
        print("âš ï¸  ì¼ì¼ ë³´ê³ ì„œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\n")
        return 0
    
    all_chunks = []
    
    for idx, file_path in enumerate(txt_files, 1):
        print(f"[{idx}/{len(txt_files)}] ì²˜ë¦¬ ì¤‘: {file_path.name}")
        
        raw_json = parse_json_file(file_path)
        if not raw_json:
            print(f"  âš ï¸  JSON íŒŒì‹± ì‹¤íŒ¨")
            continue
        
        try:
            # Raw â†’ Canonical ë³€í™˜
            canonical = service.normalize_daily(raw_json)
            
            # ì˜ë¯¸ ë‹¨ìœ„ ì²­í‚¹
            chunks = chunk_canonical_report(canonical)
            
            if not chunks:
                print(f"  âš ï¸  ì²­í¬ ìƒì„± ì‹¤íŒ¨")
                continue
            
            # ë©”íƒ€ë°ì´í„° ì •ë¦¬ (None ê°’ ì œê±°)
            for chunk in chunks:
                metadata = chunk["metadata"]
                metadata_cleaned = {k: v for k, v in metadata.items() if v is not None}
                chunk["metadata"] = metadata_cleaned
            
            all_chunks.extend(chunks)
            print(f"  âœ… {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
        
        except Exception as e:
            print(f"  âŒ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    if not all_chunks:
        print("\nâš ï¸  ìƒì„±ëœ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.\n")
        return 0
    
    # ì„ë² ë”© ë° ì €ì¥
    print(f"\nâ³ ì„ë² ë”© ìƒì„± ì¤‘... (ì´ {len(all_chunks)}ê°œ ì²­í¬)")
    texts = [chunk["text"] for chunk in all_chunks]
    embeddings = embedding_pipeline.embed_texts(texts, batch_size=BATCH_SIZE)
    
    print(f"âœ… {len(embeddings)}ê°œ ì„ë² ë”© ìƒì„± ì™„ë£Œ")
    print(f"â³ ChromaDB ì €ì¥ ì¤‘...")
    vector_store.insert_chunks(all_chunks, embeddings)
    
    collection = vector_store.get_collection()
    total_count = collection.count()
    
    print(f"âœ… ì €ì¥ ì™„ë£Œ (ì´ ë¬¸ì„œ ìˆ˜: {total_count}ê°œ)\n")
    
    return len(all_chunks)


def ingest_weekly_reports(
    service: ReportProcessingService,
    embedding_pipeline: EmbeddingPipeline,
    vector_store
) -> int:
    """ì£¼ê°„ ë³´ê³ ì„œ ingestion (í–¥í›„ êµ¬í˜„)"""
    print("\n" + "=" * 80)
    print("ğŸ“… ì£¼ê°„ ë³´ê³ ì„œ Ingestion")
    print("=" * 80)
    print("âš ï¸  ì£¼ê°„ ë³´ê³ ì„œ ëª©ì—… ë°ì´í„°ê°€ ì•„ì§ ì—†ìŠµë‹ˆë‹¤.\n")
    return 0


def ingest_monthly_reports(
    service: ReportProcessingService,
    embedding_pipeline: EmbeddingPipeline,
    vector_store
) -> int:
    """ì›”ê°„ ë³´ê³ ì„œ ingestion (í–¥í›„ êµ¬í˜„)"""
    print("\n" + "=" * 80)
    print("ğŸ“… ì›”ê°„ ë³´ê³ ì„œ Ingestion")
    print("=" * 80)
    print("âš ï¸  ì›”ê°„ ë³´ê³ ì„œ ëª©ì—… ë°ì´í„°ê°€ ì•„ì§ ì—†ìŠµë‹ˆë‹¤.\n")
    return 0


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("=" * 80)
    print("ğŸš€ ëª©ì—… ë°ì´í„° Ingestion ì‹œì‘")
    print("=" * 80)
    print(f"ğŸ“ ë°ì´í„° ê²½ë¡œ: {MOCK_DATA_DIR}")
    print(f"ğŸ’¾ ChromaDB ì €ì¥ ê²½ë¡œ: {project_root / 'Data' / 'ChromaDB' / 'report'}")
    print()
    
    # ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("âŒ OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return
    
    service = ReportProcessingService(api_key=api_key)
    
    # Vector Store ì´ˆê¸°í™”
    vector_store = get_report_vector_store()
    
    # ì„ë² ë”© íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” (ë³´ê³ ì„œ ì „ìš© vector store ì‚¬ìš©)
    embedding_pipeline = EmbeddingPipeline(vector_store=vector_store)
    
    # ê° ë³´ê³ ì„œ íƒ€ì…ë³„ ingestion
    total_chunks = 0
    
    total_chunks += ingest_daily_reports(service, embedding_pipeline, vector_store)
    total_chunks += ingest_weekly_reports(service, embedding_pipeline, vector_store)
    total_chunks += ingest_monthly_reports(service, embedding_pipeline, vector_store)
    
    # ìµœì¢… ìš”ì•½
    print("=" * 80)
    print("âœ… Ingestion ì™„ë£Œ!")
    print("=" * 80)
    print(f"ğŸ“Š ì´ {total_chunks}ê°œ ì²­í¬ê°€ ChromaDBì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    collection = vector_store.get_collection()
    print(f"ğŸ“¦ ChromaDB ì´ ë¬¸ì„œ ìˆ˜: {collection.count()}ê°œ")
    print(f"ğŸ’¾ ì €ì¥ ìœ„ì¹˜: {project_root / 'Data' / 'ChromaDB' / 'report'}")
    print()


if __name__ == "__main__":
    main()

